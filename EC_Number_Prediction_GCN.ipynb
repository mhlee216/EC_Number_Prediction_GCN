{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch.nn import BatchNorm1d\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "import copy\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score, f1_score\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from Bio.PDB.Polypeptide import three_to_one\n",
    "import biotite.structure as struc\n",
    "import biotite.structure.io as strucio\n",
    "import biotite.database.rcsb as rcsb\n",
    "from biotite.structure import filter_amino_acids\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sn\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.527516Z",
     "iopub.status.busy": "2021-04-01T09:18:14.526773Z",
     "iopub.status.idle": "2021-04-01T09:18:14.532066Z",
     "shell.execute_reply": "2021-04-01T09:18:14.532546Z"
    }
   },
   "outputs": [],
   "source": [
    "paser = argparse.ArgumentParser()\n",
    "args = paser.parse_args(\"\")\n",
    "args.seed = 123\n",
    "args.test_size = 0.2\n",
    "args.shuffle = True\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.534832Z",
     "iopub.status.busy": "2021-04-01T09:18:14.534105Z",
     "iopub.status.idle": "2021-04-01T09:18:14.538229Z",
     "shell.execute_reply": "2021-04-01T09:18:14.538704Z"
    }
   },
   "outputs": [],
   "source": [
    "H1_dict = {'A' : 0.62, \n",
    "           'C' : 0.29, \n",
    "           'D' : -0.9, \n",
    "           'E' : -0.74, \n",
    "           'F' : 1.19, \n",
    "           'G' : 0.48, \n",
    "           'H' : -0.4, \n",
    "           'I' : 1.38, \n",
    "           'K' : -1.5, \n",
    "           'L' : 1.06, \n",
    "           'M' : 0.64, \n",
    "           'N' : -0.78, \n",
    "           'P' : 0.12, \n",
    "           'Q' : -0.85, \n",
    "           'R' : -2.53, \n",
    "           'S' : -0.18, \n",
    "           'T' : -0.05, \n",
    "           'V' : 1.08, \n",
    "           'W' : 0.81, \n",
    "           'Y' : 0.26}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.540863Z",
     "iopub.status.busy": "2021-04-01T09:18:14.540145Z",
     "iopub.status.idle": "2021-04-01T09:18:14.544213Z",
     "shell.execute_reply": "2021-04-01T09:18:14.544681Z"
    }
   },
   "outputs": [],
   "source": [
    "H2_dict = {'A' : -0.5, \n",
    "           'C' : -1, \n",
    "           'D' : 3, \n",
    "           'E' : 3, \n",
    "           'F' : -2.5, \n",
    "           'G' : 0, \n",
    "           'H' : -0.5, \n",
    "           'I' : -1.8, \n",
    "           'K' : 3, \n",
    "           'L' : -1.8, \n",
    "           'M' : -1.3, \n",
    "           'N' : 2, \n",
    "           'P' : 0, \n",
    "           'Q' : 0.2, \n",
    "           'R' : 3, \n",
    "           'S' : 0.3, \n",
    "           'T' : -0.4, \n",
    "           'V' : -1.5, \n",
    "           'W' : -3.4, \n",
    "           'Y' : -2.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.546882Z",
     "iopub.status.busy": "2021-04-01T09:18:14.546164Z",
     "iopub.status.idle": "2021-04-01T09:18:14.550094Z",
     "shell.execute_reply": "2021-04-01T09:18:14.550560Z"
    }
   },
   "outputs": [],
   "source": [
    "PL_dict = {'A' : 8.1, \n",
    "           'C' : 5.5, \n",
    "           'D' : 13, \n",
    "           'E' : 12.3, \n",
    "           'F' : 5.2, \n",
    "           'G' : 9, \n",
    "           'H' : 10.4, \n",
    "           'I' : 5.2, \n",
    "           'K' : 11.3, \n",
    "           'L' : 4.9, \n",
    "           'M' : 5.7, \n",
    "           'N' : 11.6, \n",
    "           'P' : 8, \n",
    "           'Q' : 10.5, \n",
    "           'R' : 10.5, \n",
    "           'S' : 9.2, \n",
    "           'T' : 8.6, \n",
    "           'V' : 5.9, \n",
    "           'W' : 5.4, \n",
    "           'Y' : 6.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.552722Z",
     "iopub.status.busy": "2021-04-01T09:18:14.551999Z",
     "iopub.status.idle": "2021-04-01T09:18:14.555955Z",
     "shell.execute_reply": "2021-04-01T09:18:14.556424Z"
    }
   },
   "outputs": [],
   "source": [
    "SASA_dict = {'A' : 1.181, \n",
    "             'C' : 1.461, \n",
    "             'D' : 1.587, \n",
    "             'E' : 1.862, \n",
    "             'F' : 2.228, \n",
    "             'G' : 0.881, \n",
    "             'H' : 2.025, \n",
    "             'I' : 1.81, \n",
    "             'K' : 2.258, \n",
    "             'L' : 1.931, \n",
    "             'M' : 2.034, \n",
    "             'N' : 1.655, \n",
    "             'P' : 1.468, \n",
    "             'Q' : 1.932, \n",
    "             'R' : 2.56, \n",
    "             'S' : 1.298, \n",
    "             'T' : 1.525, \n",
    "             'V' : 1.645, \n",
    "             'W' : 2.663, \n",
    "             'Y' : 2.368}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.558622Z",
     "iopub.status.busy": "2021-04-01T09:18:14.557905Z",
     "iopub.status.idle": "2021-04-01T09:18:14.561977Z",
     "shell.execute_reply": "2021-04-01T09:18:14.562449Z"
    }
   },
   "outputs": [],
   "source": [
    "pKa_dict = {'A' : 2.34,\n",
    "            'R' : 2.17,\n",
    "            'N' : 2.02,\n",
    "            'D' : 1.88,\n",
    "            'C' : 1.96,\n",
    "            'E' : 2.19,\n",
    "            'Q' : 2.17,\n",
    "            'G' : 2.34,\n",
    "            'H' : 1.82,\n",
    "            'O' : 1.82,\n",
    "            'I' : 2.36,\n",
    "            'L' : 2.36,\n",
    "            'K' : 2.18,\n",
    "            'M' : 2.28,\n",
    "            'F' : 1.83,\n",
    "            'P' : 1.99,\n",
    "            'U' : 0,\n",
    "            'S' : 2.21,\n",
    "            'T' : 2.09,\n",
    "            'W' : 2.83,\n",
    "            'Y' : 2.20,\n",
    "            'V' : 2.32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.564589Z",
     "iopub.status.busy": "2021-04-01T09:18:14.563887Z",
     "iopub.status.idle": "2021-04-01T09:18:14.567927Z",
     "shell.execute_reply": "2021-04-01T09:18:14.568397Z"
    }
   },
   "outputs": [],
   "source": [
    "pKb_dict = {'A' : 9.69,\n",
    "            'R' : 9.04,\n",
    "            'N' : 8.80,\n",
    "            'D' : 9.60,\n",
    "            'C' : 10.28,\n",
    "            'E' : 9.67,\n",
    "            'Q' : 9.13,\n",
    "            'G' : 9.60,\n",
    "            'H' : 9.17,\n",
    "            'O' : 9.65,\n",
    "            'I' : 9.60,\n",
    "            'L' : 9.60,\n",
    "            'K' : 8.95,\n",
    "            'M' : 9.21,\n",
    "            'F' : 9.13,\n",
    "            'P' : 10.60,\n",
    "            'U' : 0,\n",
    "            'S' : 9.15,\n",
    "            'T' : 9.10,\n",
    "            'W' : 9.39,\n",
    "            'Y' : 9.11,\n",
    "            'V' : 9.62}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.570587Z",
     "iopub.status.busy": "2021-04-01T09:18:14.569871Z",
     "iopub.status.idle": "2021-04-01T09:18:14.573921Z",
     "shell.execute_reply": "2021-04-01T09:18:14.574388Z"
    }
   },
   "outputs": [],
   "source": [
    "pI_dict = {'A' : 6.00,\n",
    "           'R' : 10.76,\n",
    "           'N' : 5.41,\n",
    "           'D' : 2.77,\n",
    "           'C' : 5.07,\n",
    "           'E' : 3.22,\n",
    "           'Q' : 5.65,\n",
    "           'G' : 5.97,\n",
    "           'H' : 7.59,\n",
    "           'O' : 0,\n",
    "           'I' : 6.02,\n",
    "           'L' : 5.98,\n",
    "           'K' : 9.74,\n",
    "           'M' : 5.74,\n",
    "           'F' : 5.48,\n",
    "           'P' : 6.30,\n",
    "           'U' : 5.68,\n",
    "           'S' : 5.68,\n",
    "           'T' : 5.60,\n",
    "           'W' : 5.89,\n",
    "           'Y' : 5.66,\n",
    "           'V' : 5.96}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:18:14.576840Z",
     "iopub.status.busy": "2021-04-01T09:18:14.575967Z",
     "iopub.status.idle": "2021-04-01T09:18:14.601433Z",
     "shell.execute_reply": "2021-04-01T09:18:14.601907Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "AA = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "def aa_features(x):\n",
    "    return np.array(one_of_k_encoding(x, AA) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).aromaticity()), [0, 1]) +  \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).isoelectric_point()), [4, 5, 6, 8, 9]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).gravy()), [0, 1, 2, 3, 4, -4, -3, -1]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).secondary_structure_fraction()[0]), [0, 1]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).secondary_structure_fraction()[1]), [0, 1]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).secondary_structure_fraction()[2]), [0, 1]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).molar_extinction_coefficient()[0]), [0, 1490, 5500]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).molar_extinction_coefficient()[1]), [0, 1490, 5500]) + \n",
    "                    one_of_k_encoding(int(ProteinAnalysis(x).molecular_weight()), [121, 131, 132, 165, 133, 105, 75, 204, 174, \n",
    "                                                                                   146, 115, 147, 149, 117, 119, 181, 89, 155]) + \n",
    "                    one_of_k_encoding(H1_dict[x], list(set(list(H1_dict.values())))) + \n",
    "                    one_of_k_encoding(H2_dict[x], list(set(list(H2_dict.values())))) + \n",
    "                    one_of_k_encoding(PL_dict[x], list(set(list(PL_dict.values())))) + \n",
    "                    one_of_k_encoding(SASA_dict[x], list(set(list(SASA_dict.values())))) + \n",
    "                    one_of_k_encoding(pKa_dict[x], list(set(list(pKa_dict.values())))) + \n",
    "                    one_of_k_encoding(pKb_dict[x], list(set(list(pKb_dict.values())))) + \n",
    "                    one_of_k_encoding(pI_dict[x], list(set(list(pI_dict.values())))))\n",
    "\n",
    "def adjacency2edgeindex(adjacency):\n",
    "    start = []\n",
    "    end = []\n",
    "    adjacency = adjacency - np.eye(adjacency.shape[0], dtype=int)\n",
    "    for x in range(adjacency.shape[1]):\n",
    "        for y in range(adjacency.shape[0]):\n",
    "            if adjacency[x, y] == 1:\n",
    "                start.append(x)\n",
    "                end.append(y)\n",
    "\n",
    "    edge_index = np.asarray([start, end])\n",
    "    return edge_index\n",
    "\n",
    "AMINOS =  ['CYS', 'ASP', 'SER', 'GLN', 'LYS', 'ILE', 'PRO', 'THR', 'PHE', 'ASN', \n",
    "           'GLY', 'HIS', 'LEU', 'ARG', 'TRP', 'ALA', 'VAL', 'GLU', 'TYR', 'MET']\n",
    "def filter_20_amino_acids(array):\n",
    "    return ( np.in1d(array.res_name, AMINOS) & (array.res_id != -1) )\n",
    "\n",
    "# RNA Graph (1Q8N) \n",
    "# filter_20_amino_acids -> filter_seq\n",
    "SEQ = ['A', 'T', 'G', 'C', 'U']\n",
    "def filter_seq(array):\n",
    "    return ( np.in1d(array.res_name, SEQ) & (array.res_id != -1) )\n",
    "\n",
    "def protein_analysis(pdb_id):\n",
    "    file_name = rcsb.fetch(pdb_id, \"mmtf\", './data/pdb')\n",
    "    array = strucio.load_structure(file_name)\n",
    "#     protein_mask = filter_amino_acids(array)\n",
    "    protein_mask = filter_20_amino_acids(array)\n",
    "    try:\n",
    "        array = array[protein_mask]\n",
    "    except:\n",
    "        array = array[0]\n",
    "        array = array[protein_mask]\n",
    "    try:\n",
    "        ca = array[array.atom_name == \"CA\"]\n",
    "    except:\n",
    "        array = array[0]\n",
    "        ca = array[array.atom_name == \"CA\"]\n",
    "    \n",
    "    seq = ''.join([three_to_one(str(i).split(' CA')[0][-3:]) for i in ca])\n",
    "    # 7 Angstrom adjacency threshold\n",
    "    threshold = 7\n",
    "    # Create cell list of the CA atom array\n",
    "    # for efficient measurement of adjacency\n",
    "    cell_list = struc.CellList(ca, cell_size=threshold)\n",
    "    A = cell_list.create_adjacency_matrix(threshold)\n",
    "    A = np.where(A == True, 1, A)\n",
    "\n",
    "    return [aa_features(aa) for aa in seq], adjacency2edgeindex(A)\n",
    "\n",
    "def pro2vec(pdb_id):\n",
    "    node_f, edge_index = protein_analysis(pdb_id)\n",
    "    data = Data(x=torch.tensor(node_f, dtype=torch.float),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.long))\n",
    "    # print(data)\n",
    "    return data\n",
    "\n",
    "def make_pro(df):\n",
    "    pro_key = []\n",
    "    pro_value = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        pro_key.append(df['PDB ID'].iloc[i])\n",
    "        pro_value.append(df['Class'].iloc[i])\n",
    "    return pro_key, pro_value\n",
    "\n",
    "def save_graph(graph_path, pdb_id):\n",
    "    vec = pro2vec(pdb_id)\n",
    "    np.save(graph_path+pdb_id+'_e.npy', vec.edge_index)\n",
    "    np.save(graph_path+pdb_id+'_n.npy', vec.x)\n",
    "    \n",
    "def load_graph(graph_path, pdb_id):\n",
    "    n = np.load(graph_path+pdb_id+'_n.npy')\n",
    "    e = np.load(graph_path+pdb_id+'_e.npy')\n",
    "    N = torch.tensor(n, dtype=torch.float)\n",
    "    E = torch.tensor(e, dtype=torch.long)\n",
    "    data = Data(x=N, edge_index=E)\n",
    "    return data\n",
    "\n",
    "def make_vec(pro, value, class_size):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in tqdm(range(len(pro))):\n",
    "        m = pro[i]\n",
    "        y = value[i]\n",
    "        try:\n",
    "            v = load_graph('./data/graph/', m)\n",
    "            if v.x.shape[0] < 100000:\n",
    "                X.append(v)\n",
    "                Y.append(y)\n",
    "        except:\n",
    "            continue\n",
    "    for i, data in enumerate(X):\n",
    "        y = Y[i]      \n",
    "        y = np.array([int(i) for i in y.split('/')])\n",
    "        data.y = torch.tensor(y)\n",
    "    return X\n",
    "\n",
    "def df_check(df):\n",
    "    df['pro2vec'] = 'Yes'\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        try:\n",
    "            save_graph('./data/graph/', df['PDB ID'].iloc[i])\n",
    "        except:\n",
    "            df['pro2vec'].iloc[i] = 'No'\n",
    "            continue\n",
    "    df = df[df['pro2vec'] != 'No'].reset_index(drop=True)\n",
    "    del df['pro2vec']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# graph = pro2vec('3ETU')\n",
    "# G = to_networkx(graph)\n",
    "# plt.figure(1,figsize=(10, 10)) \n",
    "# nx.draw(G, node_size=50,linewidths=6)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "https://github.com/baranwa2/Struct2Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./PDB_EnzyNet_Data.csv')\n",
    "# df = df_check(df)\n",
    "# df.to_csv('./PDB_EnzyNet_Dataset.csv', index=False)\n",
    "df = pd.read_csv('./PDB_EnzyNet_Dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = []\n",
    "for ec in df['EC']:\n",
    "    for i in ec[1:-1].split(', '):\n",
    "        class_list += i.split('/')\n",
    "    \n",
    "list(set(class_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_size = len(list(set(class_list)))\n",
    "class_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'] = 0\n",
    "for i in tqdm(range(df.shape[0])):\n",
    "    ec = [int(i) for i in df['EC'].iloc[i][1:-1].split(', ')]\n",
    "    cl = [0 for i in range(class_size)]\n",
    "    for e in ec:\n",
    "        cl[e-1] = 1\n",
    "    df['Class'].iloc[i] = '/'.join([str(i) for i in cl])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {}\n",
    "for i in range(class_size):\n",
    "    class_dict[i+1] = class_list.count(str(i+1))\n",
    "print(class_dict)\n",
    "\n",
    "# plt.bar(range(len(class_dict)), list(class_dict.values()), color='blue')\n",
    "# plt.xticks(range(len(class_dict)), list(class_dict.keys()))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df, test_size=0.2, random_state=2021)#, stratify=df['Class'])\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = []\n",
    "for ec in X_train['EC']:\n",
    "    for i in ec[1:-1].split(', '):\n",
    "        class_list += i.split('/')\n",
    "\n",
    "class_dict = {}\n",
    "for i in range(class_size):\n",
    "    class_dict[i+1] = class_list.count(str(i+1))\n",
    "print(class_dict)\n",
    "\n",
    "# plt.bar(range(len(class_dict)), list(class_dict.values()), color='blue')\n",
    "# plt.xticks(range(len(class_dict)), list(class_dict.keys()))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = []\n",
    "for ec in X_test['EC']:\n",
    "    for i in ec[1:-1].split(', '):\n",
    "        class_list += i.split('/')\n",
    "\n",
    "class_dict = {}\n",
    "for i in range(class_size):\n",
    "    class_dict[i+1] = class_list.count(str(i+1))\n",
    "print(class_dict)\n",
    "\n",
    "# plt.bar(range(len(class_dict)), list(class_dict.values()), color='blue')\n",
    "# plt.xticks(range(len(class_dict)), list(class_dict.keys()))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pro_key, train_pro_value = make_pro(X_train)\n",
    "test_pro_key, test_pro_value = make_pro(X_test)\n",
    "\n",
    "train_X = make_vec(train_pro_key, train_pro_value, class_size)\n",
    "test_X = make_vec(test_pro_key, test_pro_value, class_size)\n",
    "\n",
    "print('- Train Data :', len(train_X))\n",
    "print('- Test Data :', len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-01T09:53:05.412072Z",
     "iopub.status.busy": "2021-04-01T09:53:05.411269Z",
     "iopub.status.idle": "2021-04-01T09:53:05.425685Z",
     "shell.execute_reply": "2021-04-01T09:53:05.426170Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCNlayer(nn.Module):\n",
    "    def __init__(self, n_features, conv_dim1, conv_dim2, conv_dim3, concat_dim, dropout):\n",
    "        super(GCNlayer, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.conv_dim1 = conv_dim1\n",
    "        self.conv_dim2 = conv_dim2\n",
    "        self.conv_dim3 = conv_dim3\n",
    "        self.concat_dim =  concat_dim\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.conv1 = GCNConv(self.n_features, self.conv_dim1, cached=False)\n",
    "        nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        self.bn1 = BatchNorm1d(self.conv_dim1)\n",
    "        self.conv2 = GCNConv(self.conv_dim1, self.conv_dim2, cached=False)\n",
    "        nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        self.bn2 = BatchNorm1d(self.conv_dim2)\n",
    "        self.conv3 = GCNConv(self.conv_dim2, self.conv_dim3, cached=False)\n",
    "        nn.init.xavier_uniform_(self.conv3.weight)\n",
    "        self.bn3 = BatchNorm1d(self.conv_dim3)\n",
    "        self.conv4 = GCNConv(self.conv_dim3, self.concat_dim, cached=False)\n",
    "        nn.init.xavier_uniform_(self.conv4.weight)\n",
    "        self.bn4 = BatchNorm1d(self.concat_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = global_add_pool(x, data.batch)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "    \n",
    "class FClayer(nn.Module):\n",
    "    def __init__(self, concat_dim, pred_dim1, pred_dim2, pred_dim3, out_dim, dropout):\n",
    "        super(FClayer, self).__init__()\n",
    "        self.concat_dim = concat_dim\n",
    "        self.pred_dim1 = pred_dim1\n",
    "        self.pred_dim2 = pred_dim2\n",
    "        self.pred_dim3 = pred_dim3\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.fc1 = Linear(self.concat_dim, self.pred_dim1)\n",
    "        self.bn1 = BatchNorm1d(self.pred_dim1)\n",
    "        self.fc2 = Linear(self.pred_dim1, self.pred_dim2)\n",
    "        self.bn2 = BatchNorm1d(self.pred_dim2)\n",
    "        self.fc3 = Linear(self.pred_dim2, self.pred_dim3)\n",
    "        self.fc4 = Linear(self.pred_dim3, self.out_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = F.relu(self.fc1(data))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNlayer(args.n_features, \n",
    "                              args.conv_dim1, \n",
    "                              args.conv_dim2, \n",
    "                              args.conv_dim3, \n",
    "                              args.concat_dim, \n",
    "                              args.dropout)\n",
    "\n",
    "        self.fc1 = FClayer(args.concat_dim, \n",
    "                           args.pred_dim1, \n",
    "                           args.pred_dim2, \n",
    "                           args.pred_dim3, \n",
    "                           args.out_dim, \n",
    "                           args.dropout)\n",
    "        self.fc2 = FClayer(args.concat_dim, \n",
    "                           args.pred_dim1, \n",
    "                           args.pred_dim2, \n",
    "                           args.pred_dim3, \n",
    "                           args.out_dim, \n",
    "                           args.dropout)\n",
    "        self.fc3 = FClayer(args.concat_dim, \n",
    "                           args.pred_dim1, \n",
    "                           args.pred_dim2, \n",
    "                           args.pred_dim3, \n",
    "                           args.out_dim, \n",
    "                           args.dropout)\n",
    "        self.fc4 = FClayer(args.concat_dim, \n",
    "                           args.pred_dim1, \n",
    "                           args.pred_dim2, \n",
    "                           args.pred_dim3, \n",
    "                           args.out_dim, \n",
    "                           args.dropout)\n",
    "        self.fc5 = FClayer(args.concat_dim, \n",
    "                           args.pred_dim1, \n",
    "                           args.pred_dim2, \n",
    "                           args.pred_dim3, \n",
    "                           args.out_dim, \n",
    "                           args.dropout)\n",
    "        self.fc6 = FClayer(args.concat_dim, \n",
    "                           args.pred_dim1, \n",
    "                           args.pred_dim2, \n",
    "                           args.pred_dim3, \n",
    "                           args.out_dim, \n",
    "                           args.dropout)\n",
    "#         self.fc7 = FClayer(args.concat_dim, \n",
    "#                            args.pred_dim1, \n",
    "#                            args.pred_dim2, \n",
    "#                            args.pred_dim3, \n",
    "#                            args.out_dim, \n",
    "#                            args.dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, pro):\n",
    "        x = self.conv1(pro)\n",
    "        x1 = F.log_softmax(self.fc1(x), dim = 1)\n",
    "        x2 = F.log_softmax(self.fc2(x), dim = 1)\n",
    "        x3 = F.log_softmax(self.fc3(x), dim = 1)\n",
    "        x4 = F.log_softmax(self.fc4(x), dim = 1)\n",
    "        x5 = F.log_softmax(self.fc5(x), dim = 1)\n",
    "        x6 = F.log_softmax(self.fc6(x), dim = 1)\n",
    "#         x7 = F.log_softmax(self.fc7(x), dim = 1)\n",
    "        return torch.cat([x1, x2, x3, x4, x5, x6])#, x7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learnopencv.com/multi-label-image-classification-with-pytorch-image-tagging/\n",
    "def calculate_metrics(target, pred):\n",
    "    return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples')}\n",
    "\n",
    "def make_pred(outputs):\n",
    "    return torch.tensor([torch.max(outputs[i], 1)[1].tolist() for i in range(outputs.shape[0])])\n",
    "\n",
    "def make_outputs(outputs):\n",
    "    return outputs.reshape(class_size, args.batch_size, -1)\n",
    "\n",
    "def make_labels(y):\n",
    "    return y.reshape(args.batch_size, class_size).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(outputs, labels):\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    loss_1 = loss_func(outputs[0], labels[0])\n",
    "    loss_2 = loss_func(outputs[1], labels[1])\n",
    "    loss_3 = loss_func(outputs[2], labels[2])\n",
    "    loss_4 = loss_func(outputs[3], labels[3])\n",
    "    loss_5 = loss_func(outputs[4], labels[4])\n",
    "    loss_6 = loss_func(outputs[5], labels[5])\n",
    "#     loss_7 = nn.BCELoss()(outputs[6], labels[6])\n",
    "    loss = loss_1 + loss_2 + loss_3 + loss_4 + loss_5 + loss_6# + loss_7\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, filename):\n",
    "    state = {'Epoch': epoch,\n",
    "             'State_dict': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict()}\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "def train(model, device, optimizer, train_loader, criterion, args):\n",
    "    pro_total = torch.zeros((6, 1)).int()\n",
    "    pred_pro_total = torch.zeros((6, 1)).int()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0\n",
    "    for i, pro in enumerate(train_loader):\n",
    "        pro = pro.to(device)\n",
    "        labels = make_labels(pro.y).to(device)\n",
    "        outputs = model(pro)\n",
    "        outputs = make_outputs(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_total += pro.y.size(0)\n",
    "        predicted = make_pred(outputs)\n",
    "        pro_total = torch.cat([pro_total, labels.cpu()], dim=1)\n",
    "        pred_pro_total = torch.cat([pred_pro_total, predicted], dim=1)\n",
    "        train_correct += (predicted == labels.cpu()).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    pro_total = pro_total[:,1:]\n",
    "    pred_pro_total = pred_pro_total[:,1:]\n",
    "    train_acc =  100 * train_correct / train_total\n",
    "    train_loss /= len(train_loader)\n",
    "    train_metrics = calculate_metrics(pro_total.T, pred_pro_total.T)\n",
    "    print('- Loss : %.4f' % train_loss)\n",
    "    print('- Accuracy : %.4f' % train_acc)\n",
    "    return model, train_acc, train_loss, train_metrics\n",
    "\n",
    "def test(model, device, test_loader, criterion, args):\n",
    "    model.eval()\n",
    "    pro_total = torch.zeros((6, 1)).int()\n",
    "    pred_pro_total = torch.zeros((6, 1)).int()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, pro in enumerate(test_loader):\n",
    "            pro = pro.to(device)\n",
    "            labels = make_labels(pro.y).to(device)\n",
    "            outputs = model(pro)\n",
    "            outputs = make_outputs(outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_total += pro.y.size(0)\n",
    "            predicted = make_pred(outputs)\n",
    "            test_loss += loss.item()\n",
    "            pro_total = torch.cat([pro_total, labels.cpu()], dim=1)\n",
    "            pred_pro_total = torch.cat([pred_pro_total, predicted], dim=1)\n",
    "            test_correct += (predicted == labels.cpu()).sum().item()\n",
    "    pro_total = pro_total[:,1:]\n",
    "    pred_pro_total = pred_pro_total[:,1:]\n",
    "    test_acc =  100 * test_correct / test_total\n",
    "    test_loss /= len(test_loader)\n",
    "    print('- Loss : %.4f' % test_loss)\n",
    "    print('- Accuracy : %.4f' % test_acc)\n",
    "    multi_conf = multilabel_confusion_matrix(pro_total.T, pred_pro_total.T)\n",
    "    subacc = [((multi_conf[i][0, 0] + multi_conf[i][1, 1])/multi_conf[0].sum())*100 for i in range(class_size)]\n",
    "    for s in range(len(subacc)):\n",
    "        print(f'- Class {s+1} Acc : %.4f' % subacc[s])\n",
    "    test_metrics = calculate_metrics(pro_total.T, pred_pro_total.T)\n",
    "    micro_precision = test_metrics['micro/precision']\n",
    "    micro_recall = test_metrics['micro/recall']\n",
    "    micro_f1 = test_metrics['micro/f1']\n",
    "    macro_precision = test_metrics['macro/precision']\n",
    "    macro_recall = test_metrics['macro/recall']\n",
    "    macro_f1 = test_metrics['macro/f1']\n",
    "    samples_precision = test_metrics['samples/precision']\n",
    "    samples_recall = test_metrics['samples/recall']\n",
    "    samples_f1 = test_metrics['samples/f1']\n",
    "    print('- micro_precision : %.4f' % micro_precision)\n",
    "    print('- micro_recall : %.4f' % micro_recall)\n",
    "    print('- micro_f1 : %.4f' % micro_f1)\n",
    "    print('- macro_precision : %.4f' % macro_precision)\n",
    "    print('- macro_recall : %.4f' % macro_recall)\n",
    "    print('- macro_f1 : %.4f' % macro_f1)\n",
    "    print('- samples_precision : %.4f' % samples_precision)\n",
    "    print('- samples_recall : %.4f' % samples_recall)\n",
    "    print('- samples_f1 : %.4f' % samples_f1)\n",
    "    return pro_total, pred_pro_total, test_acc, test_loss, subacc, test_metrics\n",
    "\n",
    "def experiment(model, train_loader, test_loader, device, args):\n",
    "    time_start = time.time()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=args.lr)\n",
    "    criterion = get_loss\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                          step_size=args.step_size,\n",
    "                                          gamma=args.gamma)\n",
    "    \n",
    "    list_train_acc = []\n",
    "    list_train_loss = []\n",
    "    list_test_acc = []\n",
    "    list_test_loss = []\n",
    "    list_subacc = []\n",
    "    list_train_metrics = []\n",
    "    list_test_metrics = []\n",
    "    for epoch in range(args.epoch):\n",
    "        scheduler.step()\n",
    "        print('- Epoch :', epoch+1)\n",
    "        print('[Train]')\n",
    "        model, train_ac, train_lo, train_metr = train(model, device, optimizer, train_loader, criterion, args)\n",
    "        print('[Test]')\n",
    "        _, _, test_ac, test_lo, suba, test_metr = test(model, device, test_loader, criterion, args)\n",
    "        list_train_acc.append(train_ac)\n",
    "        list_train_loss.append(train_lo)\n",
    "        list_test_acc.append(test_ac)\n",
    "        list_test_loss.append(test_lo)\n",
    "        list_subacc.append(suba)\n",
    "        list_train_metrics.append(train_metr)\n",
    "        list_test_metrics.append(test_metr)\n",
    "        print()\n",
    "    print()\n",
    "    print('[Test]')\n",
    "    pro_total, pred_pro_total, test_acc, test_loss, subacc, test_metrics = test(model, device, test_loader, criterion, args)\n",
    "    multi_conf = multilabel_confusion_matrix(pro_total.T, pred_pro_total.T)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_required = time_end - time_start\n",
    "    \n",
    "    args.list_train_acc = list_train_acc\n",
    "    args.list_train_loss = list_train_loss\n",
    "    args.list_test_acc = list_test_acc\n",
    "    args.list_test_loss = list_test_loss\n",
    "    args.list_train_metrics = list_train_metrics\n",
    "    args.list_test_metrics = list_test_metrics\n",
    "    args.multi_conf = multi_conf\n",
    "    args.test_acc = test_acc\n",
    "    args.test_loss = test_loss\n",
    "#     args.list_subacc = list_subacc\n",
    "    args.subacc = subacc\n",
    "    args.test_metrics = test_metrics\n",
    "    args.time_required = time_required\n",
    "    \n",
    "    save_checkpoint(epoch, model, optimizer, './mymodel.pt')\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(df):\n",
    "\n",
    "    train_loss = df['list_train_loss'].iloc[0]\n",
    "    train_acc = df['list_train_acc'].iloc[0]\n",
    "    test_loss = df['list_test_loss'].iloc[0]\n",
    "    test_acc = df['list_test_acc'].iloc[0]\n",
    "    accuracy = df['test_acc'].iloc[0]\n",
    "    multi_conf = df['multi_conf'].iloc[0]\n",
    "    \n",
    "    list_test_metrics = df['list_test_metrics'].iloc[0]\n",
    "    micro_precision = [m['micro/precision'] for m in list_test_metrics]\n",
    "    micro_recall = [m['micro/recall'] for m in list_test_metrics]\n",
    "    micro_f1 = [m['micro/f1'] for m in list_test_metrics]\n",
    "    macro_precision = [m['macro/precision'] for m in list_test_metrics]\n",
    "    macro_recall = [m['macro/recall'] for m in list_test_metrics]\n",
    "    macro_f1 = [m['macro/f1'] for m in list_test_metrics]\n",
    "    samples_precision = [m['samples/precision'] for m in list_test_metrics]\n",
    "    samples_recall = [m['samples/recall'] for m in list_test_metrics]\n",
    "    samples_f1 = [m['samples/f1'] for m in list_test_metrics]\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "    plt.plot([e for e in range(len(train_loss))], [float(t) for t in train_loss], label=\"train_loss\", c='blue')\n",
    "    plt.plot([e for e in range(len(test_loss))], [float(t) for t in test_loss], label=\"test_loss\", c='red')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "    plt.plot([e for e in range(len(train_acc))], [float(t)*0.01 for t in train_acc], label=\"train_acc\", c='blue')\n",
    "    plt.plot([e for e in range(len(test_acc))], [float(t)*0.01 for t in test_acc], label=\"test_acc\", c='red')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "    plt.plot([e for e in range(len(micro_precision))], [float(t) for t in micro_precision], label=\"micro_precision\", c='blue')\n",
    "    plt.plot([e for e in range(len(micro_recall))], [float(t) for t in micro_recall], label=\"micro_recall\", c='orange')\n",
    "    plt.plot([e for e in range(len(micro_f1))], [float(t) for t in micro_f1], label=\"micro_f1\", c='red')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Scores\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "    plt.plot([e for e in range(len(macro_precision))], [float(t) for t in macro_precision], label=\"macro_precision\", c='blue')\n",
    "    plt.plot([e for e in range(len(macro_recall))], [float(t) for t in macro_recall], label=\"macro_recall\", c='orange')\n",
    "    plt.plot([e for e in range(len(macro_f1))], [float(t) for t in macro_f1], label=\"macro_f1\", c='red')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Scores\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "    plt.plot([e for e in range(len(samples_precision))], [float(t) for t in samples_precision], label=\"samples_precision\", c='blue')\n",
    "    plt.plot([e for e in range(len(samples_recall))], [float(t) for t in samples_recall], label=\"samples_recall\", c='orange')\n",
    "    plt.plot([e for e in range(len(samples_f1))], [float(t) for t in samples_f1], label=\"samples_f1\", c='red')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Scores\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    \n",
    "    multi_conf = np.array(multi_conf)\n",
    "    subacc = [((multi_conf[i][0, 0] + multi_conf[i][1, 1])/multi_conf[0].sum())*100 for i in range(class_size)]\n",
    "    for i in range(len(multi_conf)):\n",
    "        print(f'- Class {i+1} Acc : %.4f' % subacc[i])\n",
    "        plt.figure(figsize = (6, 5))\n",
    "        sn.heatmap(multi_conf[i], annot=True, cmap='Blues', fmt=\"d\", annot_kws={\"size\": 20})\n",
    "        plt.tick_params(left=False, bottom=False)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.tight_layout()\n",
    "        plt.xlabel('Predicted Class', fontsize=16)\n",
    "        plt.ylabel('True Class', fontsize=16)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.yticks(rotation=0) \n",
    "        plt.show()\n",
    "\n",
    "    test_metrics = df['test_metrics'].iloc[0]\n",
    "    micro_precision = test_metrics['micro/precision']\n",
    "    micro_recall = test_metrics['micro/recall']\n",
    "    micro_f1 = test_metrics['micro/f1']\n",
    "    macro_precision = test_metrics['macro/precision']\n",
    "    macro_recall = test_metrics['macro/recall']\n",
    "    macro_f1 = test_metrics['macro/f1']\n",
    "    samples_precision = test_metrics['samples/precision']\n",
    "    samples_recall = test_metrics['samples/recall']\n",
    "    samples_f1 = test_metrics['samples/f1']\n",
    "    \n",
    "    print('- total_accuracy : %.4f' % accuracy)    \n",
    "    print('- micro_precision : %.4f' % micro_precision)\n",
    "    print('- micro_recall : %.4f' % micro_recall)\n",
    "    print('- micro_f1 : %.4f' % micro_f1)\n",
    "    print('- macro_precision : %.4f' % macro_precision)\n",
    "    print('- macro_recall : %.4f' % macro_recall)\n",
    "    print('- macro_f1 : %.4f' % macro_f1)\n",
    "    print('- samples_precision : %.4f' % samples_precision)\n",
    "    print('- samples_recall : %.4f' % samples_recall)\n",
    "    print('- samples_f1 : %.4f' % samples_f1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size = 16\n",
    "args.epoch = 500\n",
    "args.lr = 0.0000001\n",
    "args.optim = 'Adam'\n",
    "args.step_size = 10\n",
    "args.gamma = 0.9\n",
    "args.dropout = 0.2\n",
    "args.n_features = 194\n",
    "args.conv_dim1 = 128\n",
    "args.conv_dim2 = 128\n",
    "args.conv_dim3 = 128\n",
    "args.concat_dim = 128\n",
    "args.pred_dim1 = 128\n",
    "args.pred_dim2 = 128\n",
    "args.pred_dim3 = 128\n",
    "args.out_dim = 2\n",
    "\n",
    "model = Net(args)\n",
    "model = model.to(device)\n",
    "\n",
    "train_loader = DataLoader(train_X, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_X, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "dict_result = dict()\n",
    "args.exp_name = 'Test'\n",
    "result = vars(experiment(model, train_loader, test_loader, device, args))\n",
    "dict_result[args.exp_name] = copy.deepcopy(result)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "result_df = pd.DataFrame(dict_result).transpose()\n",
    "result_df.to_json('EC500.JSON', orient='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_json('EC.JSON', orient='table')\n",
    "make_plots(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
